# ClimateData
Crawling and preserving climate data

Dockets:
 -

  TODO:
   --
   - [ ] Find and download all pictures embedded on the page
   - [ ] Get document metadata (id, tracking number, date posted, RIN, etc)
   - [ ] Get 'original printed format' for the primary documents
   - [ ] Download information in the "show more details" section
   - [ ] Check each possible doc type and ensure its saved properly
   - [ ] Find/Create database to store everything, automate saving crawls directly to database (Archivers app?)
   - [ ] Hash documents for data integrity
   - [ ] Log all errors to file so they can be checked later
   - [ ] Check each document type is saved correctly

  In Progress:
   -
   - [ ] Use box api to autoupload documents --Zach

  Review/Testing:
   -

  Finished:
   -
   - [x] For each docket, start a crawl for Primary, Supporting, and Comments
   - [x] Get all links to document pages
   - [x] Automate finding how many documents in each docket
   - [x] Get links for each document and attachment on doc pages
   - [x] Download and save each document


